{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross entropy cost function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Data\n",
    "$$\\{(x^{(i)}, y^{(i)}): i=1,\\ldots,m\\}$$\n",
    "\n",
    "##### Model\n",
    "$$\n",
    "\\begin{array}{lll}\n",
    "p^{(i)}\n",
    "&=&\\mbox{softmax}(x^{(i)}W+b)\\\\\n",
    "&=&e^{x^{(i)}W+b} / \\mbox{np.sum}(e^{x^{(i)}W+b})\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "##### Likelihood function\n",
    "$$L(W,b)=\\prod_{i=1}^m\\prod_{k=0}^{9}\\left(p^{(i)}[k]\\right)^{y^{(i)}[k]}$$\n",
    "\n",
    "##### Log-Likelihood function\n",
    "$$l(W,b)=\\sum_{i=1}^m\\sum_{k=0}^{9} y^{(i)}[k]\\log p^{(i)}[k]$$\n",
    "\n",
    "##### Cost function\n",
    "$$J(W,b)=-\\frac{1}{m}\\sum_{i=1}^m\\sum_{k=0}^{9} y^{(i)}[k]\\log p^{(i)}[k]$$\n",
    "\n",
    "##### Maximum likelihood principle\n",
    "$$\\mbox{argmax}_{W,b}\\ L\\quad\\Leftrightarrow\\quad\\mbox{argmax}_{W,b}\\ l\\quad\\Leftrightarrow\\quad\\mbox{argmin}_{W,b}\\ J$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logits : \n",
      "[[ 0.  -0.1  1.2]\n",
      " [ 0.   0.1 -1.2]]\n",
      "\n",
      "probs  : \n",
      "[[0.19138922 0.17317612 0.6354346 ]\n",
      " [0.41556454 0.45926985 0.12516563]]\n",
      "\n",
      "probs2 : \n",
      "[[0.19138922 0.17317614 0.6354346 ]\n",
      " [0.41556454 0.45926985 0.12516563]]\n",
      "\n",
      "cross_entropy  :  [0.45344603 0.7781173 ]\n",
      "cross_entropy2 :  [0.45344606 0.77811736]\n",
      "\n",
      "loss  :  0.61578166\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "logits = tf.constant([[0.0, -0.1, 1.2], [0.0, 0.1, -1.2]])\n",
    "y_true = tf.constant([[0.0,  0.0, 1.0], [0.0, 1.0,  0.0]]) \n",
    "\n",
    "probs = tf.nn.softmax(logits)\n",
    "probs2 = tf.exp(logits) / tf.reshape(tf.reduce_sum(tf.exp(logits), axis=1), shape=(2,1))\n",
    "\n",
    "cross_entropy = tf.nn.softmax_cross_entropy_with_logits_v2(logits=logits,\n",
    "                                                           labels=y_true)\n",
    "cross_entropy2 = - tf.reduce_sum(y_true * tf.log(probs), axis=1)\n",
    "\n",
    "loss = tf.reduce_mean(cross_entropy)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    print(\"logits : \")\n",
    "    print(sess.run(logits))\n",
    "    print()\n",
    "    \n",
    "    print(\"probs  : \")\n",
    "    print(sess.run(probs))\n",
    "    print()\n",
    "    \n",
    "    print(\"probs2 : \")\n",
    "    print(sess.run(probs2))\n",
    "    print()\n",
    "    \n",
    "    print(\"cross_entropy  : \", sess.run(cross_entropy))\n",
    "    print(\"cross_entropy2 : \", sess.run(cross_entropy2))\n",
    "    print()\n",
    "    \n",
    "    print(\"loss  : \", sess.run(loss))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
